# core/llm_interface.py

import os
from abc import ABC, abstractmethod
from http import HTTPStatus
import dashscope
from tenacity import retry, stop_after_attempt, wait_exponential, RetryError

class LLMInterface(ABC):
    """
    Abstract Base Class for Language Model interfaces.
    
    This class defines the standard contract for interacting with any Large Language Model.
    The "Abstraction-First" design principle ensures that any agent can use any LLM
    that adheres to this interface without changing its internal logic.
    """
    
    @abstractmethod
    def invoke(self, prompt: str) -> str:
        """
        Sends a prompt to the LLM and returns the text response.
        
        Args:
            prompt: The input string to send to the language model.
            
        Returns:
            The text response generated by the model.
        """
        pass

class QwenLLM(LLMInterface):
    """
    Implementation of the LLMInterface for Alibaba Cloud's Qwen models via the DashScope API.
    
    This class handles API key management, model selection, and resilient API calls
    with exponential backoff retries.
    """
    
    def __init__(self, model: str, api_key: str = None, temperature: float = 0.3, top_p: float = 0.5, max_tokens: int = 1500):
        """
        Initializes the QwenLLM client.
        
        Args:
            model: The specific Qwen model to use (e.g., 'qwen-turbo', 'qwen-plus').
            api_key: The DashScope API key. If not provided, it will try to use the
                     'DASHSCOPE_API_KEY' environment variable.
            temperature (float): Controls randomness. Lower values make the model more deterministic. (0.0 to 2.0)
            top_p (float): Controls nucleus sampling. (0.0 to 1.0)
            max_tokens (int): The maximum number of tokens to generate in the response.
        
        Raises:
            ValueError: If the API key is not provided or found in the environment.
        """
        self.model = model
        self.temperature = temperature
        self.top_p = top_p
        self.max_tokens = max_tokens
        
        # Get API key from argument or environment variable
        self.api_key = api_key or os.getenv('DASHSCOPE_API_KEY')
        
        if not self.api_key:
            raise ValueError("DashScope API key not provided. Please pass it as an argument or set the 'DASHSCOPE_API_KEY' environment variable.")
        
        # Set the API endpoint to the international service URL.
        dashscope.base_http_api_url = 'https://dashscope-intl.aliyuncs.com/api/v1'

    @retry(
        stop=stop_after_attempt(3), 
        wait=wait_exponential(multiplier=1, min=4, max=10)
    )
    def invoke(self, prompt: str) -> str:
        """
        The method that makes the actual API call. A timeout and generation parameters have been added.
        """
        print(f"Invoking Qwen model '{self.model}' with temp={self.temperature}, top_p={self.top_p}...")
        
        messages = [{'role': 'user', 'content': prompt}]
        
        try:
            response = dashscope.Generation.call(
                model=self.model,
                messages=messages,
                result_format='message',
                api_key=self.api_key,
                timeout=60,
                temperature=self.temperature,
                top_p=self.top_p,
                max_tokens=self.max_tokens
            )

            if response.status_code == HTTPStatus.OK:
                print("Qwen API call successful.")
                return response.output.choices[0].message.content
            else:
                print(f"Error from DashScope API: Status {response.status_code}, Code: {response.code}, Message: {response.message}")
                response.raise_for_status()
                return ""

        except Exception as e:
            print(f"An unexpected error occurred while calling Qwen LLM: {e}")
            raise

# Example Usage:
if __name__ == '__main__':
    try:
        # You can now configure the LLM with the new parameters
        qwen_plus_llm = QwenLLM(
            model='qwen-plus',
            temperature=0.7,  # A bit more deterministic
            top_p=0.8,
            max_tokens=500     # Limit the response length
        ) 
        
        example_prompt = "What are the primary pros and cons of investing in NVIDIA (NVDA) in 2024?"
        
        response_text = qwen_plus_llm.invoke(example_prompt)
        
        print("\n--- LLM Response ---")
        print(response_text)
        print("--------------------")

    except ValueError as e:
        print(e)
    except RetryError as e:
        print(f"Failed to get a response from the LLM after multiple retries. Last error: {e}")
    except Exception as e:
        print(f"An unexpected error occurred: {e}")

